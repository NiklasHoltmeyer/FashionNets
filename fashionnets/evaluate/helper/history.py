import os
import zipfile
import pandas as pd
from fashionscrapper.utils.list import flatten, distinct
from tqdm.auto import tqdm


class HistoryHelper:
    """
    Helper Class to Extract and analyze all History Files from within the Result::Zips - as generated by the
    ZipResults::Callback
    """
    def __init__(self, base_path):
        self.base_path = base_path

    def aggregate_run_histories(self, run):
        run_folder = os.path.join(self.base_path, run)

        if not os.path.isdir(run_folder):
            return None

        files = os.listdir(run_folder)
        zips = filter(lambda d: d.endswith(".zip"), files)
        zips = map(lambda d: os.path.join(self.base_path, run, d), zips)
        list_of_history_lines = map(read_history, zips)

        list_of_history_lines = list(list_of_history_lines)

        lines = []
        for history in list_of_history_lines:
            for line in history:
                line = line.replace("\r", "").replace(".", ",")
                if line not in lines and len(line) > 0:
                    lines.append(line)

        if len(lines) < 1:
            return None

        header = lines[0].split(";")
        values = [l.split(";") for l in lines[1:]]
        df = pd.DataFrame(values, columns=header)
        csv_path = os.path.join(self.base_path, run, "train_aggregated.csv")
        df.to_csv(csv_path, index=False, sep=";", decimal=".")
        return run, csv_path

    def aggregate(self):
        runs = os.listdir(self.base_path)
        csv_paths = map(self.aggregate_run_histories, runs)
        csv_paths = filter(lambda d: d is not None, csv_paths)
        run_with_history = map(lambda r: (r[0], pd.read_csv(r[1], sep=";", decimal=".")), csv_paths)
        best_checkpoints = map(load_best_checkpoints, run_with_history)
        return list(best_checkpoints)

    def aggregate_as_df(self):
        aggregated_results = self.aggregate()

        def flatten_aggregated(line):
            metrics = ["min_loss", "min_val_loss"]
            values = [line[metrics[0]]["run_name"], line[metrics[0]]["total_epochs"]]

            for m in metrics:
                for k in ["value", "idx", "epoch"]:
                    values.append(line[m][k])
            return values

        header = ["run_name", "total_epoch"] + ["value", "idx", "epoch"] + ["val_value", "val_idx", "val_epoch"]
        data = [flatten_aggregated(x) for x in aggregated_results]
        data_frame = pd.DataFrame(data, columns=header)
        data_frame.to_csv(os.path.join(self.base_path, "best_runs.csv"), index=False, sep=";", decimal=".")

        return data_frame

    def extract_best_backbones(self):
        runs = os.listdir(self.base_path)
        csv_paths = map(self.aggregate_run_histories, runs)
        csv_paths = filter(lambda d: d is not None, csv_paths)
        run_with_history = map(lambda r: (r[0], pd.read_csv(r[1], sep=";", decimal=".")), csv_paths)
        best_checkpoints = list(map(load_best_checkpoints, run_with_history))

        load_checkpoints = lambda d: distinct([v["zip_file"] for v in d.values()])

        for cp in tqdm(best_checkpoints, desc="unpack"):
            run_name = (list(cp.values())[0]["run_name"])
            for _zip in load_checkpoints(cp):
                zip_path = os.path.join(self.base_path, run_name, _zip)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(os.path.join(self.base_path, run_name, _zip.replace(".zip", "")))

    def walk_backbones(self):
        for root, dirs, files in (os.walk(self.base_path)):
            if any(map(lambda d: "backbone" in d, dirs)):
                for d in dirs:
                    yield root, d, os.path.join(root, d)

    def walk_train_aggregated(self):
        for root, dirs, files in os.walk(self.base_path):
            if 'train_aggregated.csv' in files:
                yield (os.path.join(root, 'train_aggregated.csv'))

    def create_move_jobs(self):
        for root, directory, full_path in self.walk_backbones():
            dst = "./" + root.split('\\')[-2] + "/" + directory
            src = full_path
            print(f"rclone copy '{src}' '{dst}'")


def read_history(path):
    with zipfile.ZipFile(path, 'r') as archive:
        history_files = list(filter(lambda d: "history" in d, archive.namelist()))
        lines = []

        for history_file_path in history_files:
            history_file = archive.open(history_file_path)
            history_content = history_file.read().decode('UTF-8')
            lines.extend(history_content.split("\n"))

        return lines


def load_best_checkpoints(run_name_and_df):
    run_name, df = run_name_and_df
    keys = ["epoch", "loss", "val_loss"]
    data = {k: list(df[k].values) for k in keys}

    for metric in ["loss", "val_loss"]:
        values = [float(v.replace(",", ".")) for v in data.pop(metric)]
        min_value = min(values)
        min_idx = values.index(min_value)
        data["min_" + metric] = {
            "run_name": run_name,
            "value": min_value,
            "idx": min_idx,
            "epoch": data["epoch"][min_idx],
            "zip_file": f"{run_name}{str(data['epoch'][min_idx]).zfill(4)}.zip",
            "total_epochs": len(data["epoch"])
        }

    del data["epoch"]

    return data


if __name__ == "__main__":
    base_path = r"D:\masterarbeit_runs"
    history_helper = HistoryHelper(base_path)
    aggregated_df = history_helper.aggregate_as_df()
