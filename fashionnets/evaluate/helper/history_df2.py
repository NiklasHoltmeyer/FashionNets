import os
import zipfile
from pathlib import Path

import pandas as pd
from fashiondatasets.utils.list import parallel_map
from fashionscrapper.utils.list import distinct
from tqdm.auto import tqdm

from fashionnets.evaluate.helper.cbir_helper import build_similar_idxs


class HistoryHelper_DF2:
    """
    Helper Class to Extract and analyze all History Files from within the Result::Zips - as generated by the
    ZipResults::Callback
    """

    def __init__(self, base_path):
        self.base_path = base_path

    def aggregate_run_histories(self, run):
        run_folder = os.path.join(self.base_path, run)

        if not os.path.isdir(run_folder):
            return None

        files = os.listdir(run_folder)
        zips = filter(lambda d: d.endswith(".zip"), files)
        zips = map(lambda d: os.path.join(self.base_path, run, d), zips)
        list_of_history_lines = map(read_history, zips)

        list_of_history_lines = list(list_of_history_lines)

        lines = []
        for history in list_of_history_lines:
            for line in history:
                line = line.replace("\r", "").replace(".", ",")
                if line not in lines and len(line) > 0:
                    lines.append(line)

        if len(lines) < 1:
            return None

        header = lines[0].split(";")
        values = [line.split(";") for line in lines[1:]]
        df = pd.DataFrame(values, columns=header)
        csv_path = os.path.join(self.base_path, run, "train_aggregated.csv")
        df.to_csv(csv_path, index=False, sep=";", decimal=".")
        return run, csv_path

    # noinspection DuplicatedCode
    def aggregate(self):
        runs = os.listdir(self.base_path)
        csv_paths = map(self.aggregate_run_histories, runs)
        csv_paths = filter(lambda d: d is not None, csv_paths)
        run_with_history = map(lambda r: (r[0], pd.read_csv(r[1], sep=";", decimal=".")), csv_paths)
        best_checkpoints = map(load_best_checkpoints, run_with_history)
        return list(best_checkpoints)

    def aggregate_as_df(self):
        aggregated_results = self.aggregate()

        def flatten_aggregated(line):
            metrics = ["min_loss", "min_val_loss"]
            values = [line[metrics[0]]["run_name"], line[metrics[0]]["total_epochs"]]

            for m in metrics:
                for k in ["value", "idx", "epoch"]:
                    values.append(line[m][k])
            return values

        header = ["run_name", "total_epoch"] + ["value", "idx", "epoch"] + ["val_value", "val_idx", "val_epoch"]
        data = [flatten_aggregated(x) for x in aggregated_results]
        data_frame = pd.DataFrame(data, columns=header)
        data_frame.to_csv(os.path.join(self.base_path, "best_runs.csv"), index=False, sep=";", decimal=".")

        return data_frame

    def extract_best_backbones(self):
        runs = os.listdir(self.base_path)
        csv_paths = map(self.aggregate_run_histories, runs)
        csv_paths = filter(lambda d: d is not None, csv_paths)
        run_with_history = map(lambda r: (r[0], pd.read_csv(r[1], sep=";", decimal=".")), csv_paths)
        best_checkpoints = list(map(load_best_checkpoints, run_with_history))

        load_checkpoints = lambda d: distinct([v["zip_file"] for v in d.values()])

        for cp in tqdm(best_checkpoints, desc="unpack"):
            run_name = (list(cp.values())[0]["run_name"])
            for _zip in load_checkpoints(cp):
                zip_path = os.path.join(self.base_path, run_name, _zip)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(os.path.join(self.base_path, run_name, _zip.replace(".zip", "")))

    def walk_backbones(self):
        for root, dirs, files in (os.walk(self.base_path)):
            if any(map(lambda d_name: "backbone" in d_name, dirs)):
                for d in dirs:
                    yield root, d, os.path.join(root, d)

    def walk_train_aggregated(self):
        for root, dirs, files in os.walk(self.base_path):
            if 'train_aggregated.csv' in files:
                yield os.path.join(root, 'train_aggregated.csv')

    def walk_embeddings(self):
        for root, dirs, files in os.walk(self.base_path):
            for file in files:
                if file.endswith(".json") and not file.endswith("sim_idxs.json") and not "settings" in file:
                    run_name = Path(root).name
                    full_path = Path(root, file)
                    epoch = file.replace(".json", "").split("_")[-1]
                    yield run_name, epoch, full_path

    def walk_sim_idxs(self):
        for root, dirs, files in (os.walk(self.base_path)):
            for file in files:
                if file.endswith("sim_idxs.json"):
                    run_name = Path(root).name
                    epoch = file.split("_")[0]
                    full_path = Path(root, file)
                    yield run_name, epoch, full_path

    def walk_sim_idxs_jobs(self):
        embeddings = list(self.walk_embeddings())
        sim_idxs = list(self.walk_sim_idxs())
        sim_idxs_w_o_paths = list(map(lambda d: d[:2], sim_idxs))
        for run_name, epoch, embedding_path in embeddings:
            if not ((run_name, epoch) in sim_idxs_w_o_paths):
                yield run_name, epoch, embedding_path

    def create_move_jobs(self):
        for root, directory, full_path in self.walk_backbones():
            dst = "./" + root.split('\\')[-2] + "/" + directory
            src = full_path
            print(f"rclone copy '{src}' '{dst}'")

    def create_sim_idxs(self):
        missing_sim_idxs = (list(self.walk_sim_idxs_jobs()))
        use_tqdm_flag = [True] + [False] * (len(missing_sim_idxs) - 1)  # <- only using tqdm on first thread

        create_sim_idxs = lambda parameter: build_similar_idxs(*parameter[0], parameter[1])

        jobs = list(zip(missing_sim_idxs, use_tqdm_flag))

        parallel_map(jobs, create_sim_idxs, threads=os.cpu_count() // 2)  #


def read_history(path):
    with zipfile.ZipFile(path, 'r') as archive:
        history_files = list(filter(lambda d: "history" in d, archive.namelist()))
        lines = []

        for history_file_path in history_files:
            history_file = archive.open(history_file_path)
            history_content = history_file.read().decode('UTF-8')
            lines.extend(history_content.split("\n"))

        return lines


def load_best_checkpoints(run_name_and_df):
    run_name, df = run_name_and_df
    keys = ["epoch", "loss", "val_loss"]
    data = {k: list(df[k].values) for k in keys}

    for metric in ["loss", "val_loss"]:
        values = [float(v.replace(",", ".")) for v in data.pop(metric)]
        min_value = min(values)
        min_idx = values.index(min_value)
        data["min_" + metric] = {
            "run_name": run_name,
            "value": min_value,
            "idx": min_idx,
            "epoch": data["epoch"][min_idx],
            "zip_file": f"{run_name}{str(data['epoch'][min_idx]).zfill(4)}.zip",
            "total_epochs": len(data["epoch"])
        }

    del data["epoch"]

    return data


if __name__ == "__main__":
    # aggregate_run_histories
    helper = HistoryHelper_DF2(r"D:\masterarbeit_runs")
    helper.aggregate_run_histories("312_resnet50_imagenet_triplet")
