{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validate_Loss_Siamese_Model_(3)_(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NB_Fqm2yRXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb12b25b-e6a5-40d5-99a5-4fb4e44b09e8"
      },
      "source": [
        "test_ds_worker_name = \"g_t_apn\"  #<- force DL \n",
        "model_worker_name = test_ds_worker_name\n",
        "\n",
        "\n",
        "!pip uninstall -y fashion_nets\n",
        "!pip uninstall -y fashion_datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping fashion-nets as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping fashion-datasets as it is not installed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smUPHnPayV4C",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fba40af-9119-4fe9-a158-d1f010df8669"
      },
      "source": [
        "#@title Pip Installs & Imports\n",
        "#!pip uninstall -y fashion_nets\n",
        "!pip install -q git+https://github.com/NiklasHoltmeyer/FashionDatasets.git\n",
        "!pip install -q git+https://github.com/NiklasHoltmeyer/FashionNets.git\n",
        "\n",
        "try:\n",
        "  from fashionnets.train_jobs.loader.job_loader import prepare_environment, load_job_settings, history_to_csv_string\n",
        "  from fashionnets.train_jobs.loader.model_loader import load_siamese_model_from_train_job\n",
        "except:\n",
        "  from fashionnets.train_jobs.loader.job_loader import prepare_environment, load_job_settings, history_to_csv_string\n",
        "  from fashionnets.train_jobs.loader.model_loader import load_siamese_model_from_train_job\n",
        "from fashionnets.train_jobs.loader.checkpoint_loader import load_latest_checkpoint\n",
        "\n",
        "from fashionnets.models.layer.Augmentation import compose_augmentations\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 958 kB 15.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 631 kB 31.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 356 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 153 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 137 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 52.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 354 kB 43.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 354 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 954 kB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 904 kB 38.5 MB/s \n",
            "\u001b[?25h  Building wheel for fashion-datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fashionscrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fashion-nets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for webdavclient3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr4jchctyYex",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2ee87a-bf7b-4a38-d01d-272b0dd60816"
      },
      "source": [
        "#@title Prepare Environment. Download Datasets\n",
        "from fashionnets.train_jobs.loader.job_loader import add_back_bone_to_train_job\n",
        "\n",
        "environment, training_job_cfg = prepare_environment(test_ds_worker_name, debugging=False)\n",
        "import kaggle #<- Requires Secrets, therefore prepare_environment needs to be run before\n",
        "kaggle.api.authenticate()\n",
        "\n",
        "kaggle_downloader = kaggle.api.dataset_download_files #<- sadly needs to be injected\n",
        "\n",
        "train_job = load_job_settings(environment=environment, training_job_cfg=training_job_cfg, kaggle_downloader=kaggle_downloader)\n",
        "\n",
        "job_settings = add_back_bone_to_train_job(environment=environment, **training_job_cfg)\n",
        "\n",
        "def pass_(a, b, unzip):\n",
        "  return\n",
        "kaggle_downloader = pass_\n",
        "\n",
        "environment_m, training_job_cfg_m = prepare_environment(model_worker_name, debugging=False)\n",
        "train_job_m = load_job_settings(environment=environment_m, training_job_cfg=training_job_cfg_m, kaggle_downloader=kaggle_downloader)\n",
        "job_settings_m = add_back_bone_to_train_job(environment=environment_m, **training_job_cfg)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-12-16 17:56:53,036] {Environment.py:25} DEBUG - Environment: google\n",
            "Mounted at /gdrive/\n",
            "[2021-12-16 17:57:15,371] {Environment.py:69} DEBUG - Download: masterokay/deep-fashion-1-256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Modell from Remote CBIR\n",
        "from fashionnets.evaluate.EvaluateDeepFashion import Evaluate\n",
        "from fashiondatasets.deepfashion1.DeepFashion1CBIR import DeepFashion1CBIR\n",
        "from fashionnets.evaluate.helper.evaluate_helper_loader import load_model\n",
        "from fashionnets.evaluate.helper.evaluate_helper_loader import load_dataset\n",
        "\n",
        "from fashionnets.train_jobs.loader.path_loader import _load_embedding_base_path, _load_dataset_base_path\n",
        "from fashionnets.models.layer.Augmentation import compose_augmentations\n",
        "\n",
        "siamese_model, init_epoch, _callbacks = load_siamese_model_from_train_job(force_load_weights=True, load_weights=True, **train_job_m)\n",
        "print(\"init_epoch\", init_epoch)\n",
        "print(\"Callbacks\", _callbacks)\n",
        "\n",
        "base_path = _load_dataset_base_path(**job_settings)\n",
        "#base_path = base_path.replace(\"own_256\", \"deep_fashion_1_256\")\n",
        "embedding_base_path = _load_embedding_base_path(**job_settings)\n",
        "augmentation = compose_augmentations()(False)\n",
        "\n",
        "df_cbir = DeepFashion1CBIR(base_path=base_path, \n",
        "                 model = siamese_model.siamese_network.feature_extractor,\n",
        "                 embedding_path=embedding_base_path, \n",
        "                 augmentation=augmentation,\n",
        "                 image_suffix=\"_256\",\n",
        "                 batch_size=256)\n",
        "zip_path = df_cbir.bulk_embed(zip_=True)\n",
        "embedding_name = f\"emb_{job_settings_m['run_name']}_{init_epoch-1}.zip\"\n",
        "\n",
        "!mv $zip_path /gdrive/MyDrive/results/$embedding_name"
      ],
      "metadata": {
        "id": "tSKVOilP3PDS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELLp6z1lg9CS",
        "cellView": "form"
      },
      "source": [
        "#@title load all 4 Modells from Train-Job-CP_Path\n",
        "\n",
        "try:\n",
        "  del siamese_model\n",
        "except:\n",
        "  pass\n",
        "\n",
        "cp_path = train_job_m[\"path\"][\"checkpoint\"]#\"./\"\n",
        "modelle = load_model(cp_path, train_job)\n",
        "back_bone_modell = modelle[\"quadtruplet\"].siamese_network.feature_extractor\n",
        "\n",
        "datasets = load_dataset(model=back_bone_modell, job_settings=job_settings)\n",
        "\n",
        "assert sorted(datasets.keys()) == sorted(modelle.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm \n",
        "\n",
        "steps = None\n",
        "losses = list(modelle.keys())\n",
        "losses = list(reversed(losses))\n",
        "results = {}\n",
        "\n",
        "print({\"name\": job_settings_m['run_name'], \"epoch\": init_epoch-1, \"loss\": results})\n",
        "\n",
        "for loss_fn_str in tqdm(losses, desc=\"Calc Losses\"):\n",
        "  print(loss_fn_str)\n",
        "  model = modelle[loss_fn_str]\n",
        "  loss_dataset = datasets[loss_fn_str]\n",
        "\n",
        "  results[loss_fn_str] = {}\n",
        "\n",
        "  for ds_name, ds in loss_dataset.items():\n",
        "    print(ds_name)\n",
        "    loss_ =model.evaluate(ds, steps=steps)\n",
        "    results[loss_fn_str][ds_name] = loss_\n",
        "    print(results)\n",
        "\n",
        "    print(\"***\")\n",
        "    print({\"name\": job_settings_m['run_name'], \"epoch\": init_epoch-1, \"loss\": results})\n",
        "    print(\"***\")"
      ],
      "metadata": {
        "id": "3v7ZhUvlG8Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "zThNXbkYG71U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "TS5YPi4nG7tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_data = {\n",
        "    \"name\": job_settings_m['run_name'],\n",
        "    \"epoch\": init_epoch-1,\n",
        "    \"loss\": results\n",
        "}\n",
        "\n",
        "f_data\n"
      ],
      "metadata": {
        "id": "zOX5Nwrg68am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_data)"
      ],
      "metadata": {
        "id": "Xj4i_f4z6g1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f89U5_AQdYIk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}